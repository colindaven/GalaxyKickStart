{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"GalaxyKickStart GalaxyKickStart is an Ansible playbook designed for installing, testing, deploying and maintaining production-grade Galaxy instances. GalaxyKickStart playbook code is available in GitHub . In the basic configuration, the deployed Galaxy server includes: postgresql server as database backend nginx proxy slurm cluster In addition, tools and workflows can be pre-installed during the deployment.","title":"Home"},{"location":"#galaxykickstart","text":"GalaxyKickStart is an Ansible playbook designed for installing, testing, deploying and maintaining production-grade Galaxy instances. GalaxyKickStart playbook code is available in GitHub . In the basic configuration, the deployed Galaxy server includes: postgresql server as database backend nginx proxy slurm cluster In addition, tools and workflows can be pre-installed during the deployment.","title":"GalaxyKickStart"},{"location":"GKS2slurm/","text":"What is GKS2slurm ? GKS2slurm is a playbook that is played to install a multinode slurm cluster over a GalaxyKickStart single-node installation. The playbook GKS2slurm galaxyToSlurmCluster.yml was tested with multiple virtual machines (VMs) in Stratuslab , Google Cloud Engine (GCE) and Amazon Web Services (AWS) clouds. Installation of a Galaxy slurm cluster with GKS2slurm Step 1: Install a Galaxy server with GalaxyKickStart Report to the Getting Started section of this manual for the basics of GalaxyKickStart installation install any GalaxyKickStart \"flavor\" by configuring the inventory file (in inventory_files folder) and the group_vars file (in the group_vars folder) of your choice. Flavors currently available are kickstart , artimed and metavisitor but other will come soon. Alternatively, you can build you own flavor by customizing a group_vars, extrafiles file and inventory file, which will install your Galaxy tools and workflows. in Step 1, the most important thing to keep track with is to configure your target machine with an extra volume Indeed GKS2slurm has be designed so that the Galaxy slurm cluster can accumulate large amount of data in the long term, which can be more easily shared with the cluster nodes and more importantly backed up. Thus in addition of all the adaptations you will do for your own purpose (tools, workflows, etc), edit the group_vars/all file and adapt the galaxy_persistent_directory variable to your extra volume which should be already formatted and mounted: Change #persistent data galaxy_persistent_directory: /export # for IFB it's /root/mydisk, by default, /export To #persistent data galaxy_persistent_directory: /pathto/mounted/extravolume Having configured your GalaxyKickStart installation, import the extra roles (if not already done) ansible-galaxy install -r requirements_roles.yml -p roles and run the galaxy.yml playbook: ansible-playbook --inventory-file inventory_files/ your_inventory_file galaxy.yml Step 2: Check the single node Galaxy installation If the playbook was run successfully, connect to your Galaxy instance through http and check that you can login (admin@galaxy.org:admin), and that tools and workflows are correctly installed. Step 3: Moving your single node configuration to a multinode slurm configuration Start as many compute nodes you want for the slurm cluster and gather information from each node: IP address (all slurm nodes should must be accessible in the same network, ie nodes can be ping-ed from any nodes) hostname number of CPUs memory (in MB) Step 3-1 Adapt the inventory file slurm-kickstart in the inventory_files folder. [slurm_master] # adapt the following line to IP address and ssh user of the slurm master node 192.54.201.102 ansible_ssh_user=root ansible_ssh_private_key_file= ~/.ssh/mysshkey [slurm_slave] # adapt the following lnes to IP addresses and ssu users of the slum slave nodes 192.54.201.98 ansible_ssh_user=root ansible_ssh_private_key_file= ~/.ssh/mysshkey 192.54.201.99 ansible_ssh_user=root ansible_ssh_private_key_file= ~/.ssh/mysshkey 192.54.201.101 ansible_ssh_user=root ansible_ssh_private_key_file= ~/.ssh/mysshkey Step 3-2 Adapt the group_vars file slurm_master in the group_vars folder. This is done using the information gathered in step 3 # nfs sharing cluster_ip_range: 0.0.0.0/24 # replace by your ip network range # slave node specifications, adapt to your set of slave nodes slave_node_dict: - {hostname: slave-1 , CPUs: 2 , RealMemory: 7985 } - {hostname: slave-2 , CPUs: 2 , RealMemory: 7985 } - {hostname: slave-3 , CPUs: 2 , RealMemory: 7985 } Step 3-3 Adapt the group_vars file slurm_slave in the group_vars folder # adapt the following variable to the master slurm node IP address master_slurm_node_ip: 192.54.201.102 Step 3-4 Run the playbook galaxyToSlurmCluster.yml playbook. from the GalaxyKickStart directory: ansible-playbook -i inventory_files/slurm-kickstart galaxyToSlurmCluster.yml Note that if you configure multiple slave nodes without prior ssh key authentification, you can run the same command with the variable ANSIBLE_HOST_KEY_CHECKING put to False: ANSIBLE_HOST_KEY_CHECKING=False ansible-playbook -i inventory_files/slurm-kickstart galaxyToSlurmCluster.yml Checking slurm installation Connect to your master node as root and type sinfo Refer to slurm documentation for more investigation/control","title":"What is GKS2slurm ?"},{"location":"GKS2slurm/#what-is-gks2slurm","text":"GKS2slurm is a playbook that is played to install a multinode slurm cluster over a GalaxyKickStart single-node installation. The playbook GKS2slurm galaxyToSlurmCluster.yml was tested with multiple virtual machines (VMs) in Stratuslab , Google Cloud Engine (GCE) and Amazon Web Services (AWS) clouds.","title":"What is GKS2slurm ?"},{"location":"GKS2slurm/#installation-of-a-galaxy-slurm-cluster-with-gks2slurm","text":"","title":"Installation of a Galaxy slurm cluster with GKS2slurm"},{"location":"GKS2slurm/#step-1-install-a-galaxy-server-with-galaxykickstart","text":"Report to the Getting Started section of this manual for the basics of GalaxyKickStart installation install any GalaxyKickStart \"flavor\" by configuring the inventory file (in inventory_files folder) and the group_vars file (in the group_vars folder) of your choice. Flavors currently available are kickstart , artimed and metavisitor but other will come soon. Alternatively, you can build you own flavor by customizing a group_vars, extrafiles file and inventory file, which will install your Galaxy tools and workflows. in Step 1, the most important thing to keep track with is to configure your target machine with an extra volume Indeed GKS2slurm has be designed so that the Galaxy slurm cluster can accumulate large amount of data in the long term, which can be more easily shared with the cluster nodes and more importantly backed up. Thus in addition of all the adaptations you will do for your own purpose (tools, workflows, etc), edit the group_vars/all file and adapt the galaxy_persistent_directory variable to your extra volume which should be already formatted and mounted: Change #persistent data galaxy_persistent_directory: /export # for IFB it's /root/mydisk, by default, /export To #persistent data galaxy_persistent_directory: /pathto/mounted/extravolume Having configured your GalaxyKickStart installation, import the extra roles (if not already done) ansible-galaxy install -r requirements_roles.yml -p roles and run the galaxy.yml playbook: ansible-playbook --inventory-file inventory_files/ your_inventory_file galaxy.yml","title":"Step 1: Install a Galaxy server with GalaxyKickStart"},{"location":"GKS2slurm/#step-2-check-the-single-node-galaxy-installation","text":"If the playbook was run successfully, connect to your Galaxy instance through http and check that you can login (admin@galaxy.org:admin), and that tools and workflows are correctly installed.","title":"Step 2: Check the single node Galaxy installation"},{"location":"GKS2slurm/#step-3-moving-your-single-node-configuration-to-a-multinode-slurm-configuration","text":"Start as many compute nodes you want for the slurm cluster and gather information from each node: IP address (all slurm nodes should must be accessible in the same network, ie nodes can be ping-ed from any nodes) hostname number of CPUs memory (in MB)","title":"Step 3: Moving your single node configuration to a multinode slurm configuration"},{"location":"GKS2slurm/#step-3-1","text":"Adapt the inventory file slurm-kickstart in the inventory_files folder. [slurm_master] # adapt the following line to IP address and ssh user of the slurm master node 192.54.201.102 ansible_ssh_user=root ansible_ssh_private_key_file= ~/.ssh/mysshkey [slurm_slave] # adapt the following lnes to IP addresses and ssu users of the slum slave nodes 192.54.201.98 ansible_ssh_user=root ansible_ssh_private_key_file= ~/.ssh/mysshkey 192.54.201.99 ansible_ssh_user=root ansible_ssh_private_key_file= ~/.ssh/mysshkey 192.54.201.101 ansible_ssh_user=root ansible_ssh_private_key_file= ~/.ssh/mysshkey","title":"Step 3-1"},{"location":"GKS2slurm/#step-3-2","text":"Adapt the group_vars file slurm_master in the group_vars folder. This is done using the information gathered in step 3 # nfs sharing cluster_ip_range: 0.0.0.0/24 # replace by your ip network range # slave node specifications, adapt to your set of slave nodes slave_node_dict: - {hostname: slave-1 , CPUs: 2 , RealMemory: 7985 } - {hostname: slave-2 , CPUs: 2 , RealMemory: 7985 } - {hostname: slave-3 , CPUs: 2 , RealMemory: 7985 }","title":"Step 3-2"},{"location":"GKS2slurm/#step-3-3","text":"Adapt the group_vars file slurm_slave in the group_vars folder # adapt the following variable to the master slurm node IP address master_slurm_node_ip: 192.54.201.102","title":"Step 3-3"},{"location":"GKS2slurm/#step-3-4","text":"Run the playbook galaxyToSlurmCluster.yml playbook. from the GalaxyKickStart directory: ansible-playbook -i inventory_files/slurm-kickstart galaxyToSlurmCluster.yml Note that if you configure multiple slave nodes without prior ssh key authentification, you can run the same command with the variable ANSIBLE_HOST_KEY_CHECKING put to False: ANSIBLE_HOST_KEY_CHECKING=False ansible-playbook -i inventory_files/slurm-kickstart galaxyToSlurmCluster.yml","title":"Step 3-4"},{"location":"GKS2slurm/#checking-slurm-installation","text":"Connect to your master node as root and type sinfo Refer to slurm documentation for more investigation/control","title":"Checking slurm installation"},{"location":"GKSfromWorflows/","text":"GKSfromWorflows GKSfromWorflows uses the python script galaxykickstart_from_workflow.py to quickly generate a GalaxyKickStart use case from one or several workflow files ( .ga , note that these files must have been generated with galaxy = release_16.04) From GalaxyKickStart/scripts, run python galaxykickstart_from_workflow.py --help Then python galaxykickstart_from_workflow.py -w workflow1.ga workflow2.ga ... -l Panel_label This creates: An inventory file GKSfromWorkflow in the inventory_files folder A group_vars file GKSfromWorkflow in the group_vars folder A folder GKSfromWorkflow in the folder extra-files which will contain a copy of the workflow1.ga, workflow2.ga, ... files, plus a GKSfromWorkflow_tool_list.yml file that contains a yml description of all tools used in the workflows. Note that running galaxykickstart_from_workflow.py overwrites these folders and files if they exist from a previous script run. Adapt the created inventory file Before running ansible-playbook, you have just to adapt the GKSfromWorkflow inventory_file inventory_files/GKSfromWorkflow to your own network settings (the file is preconfigured for running locally ansible-playbook on your target machine). As usual, you may also tune the group_vars/all file. Run the playbook cd GalaxyKickStart ansible-galaxy install -r requirements_roles.yml -p roles ansible-playbook -i inventory_files/GKSfromWorkflow galaxy.yml Check your running Galaxy instance after completion of the playbook. It contains the preinstalled tools as well as the workflow1, workflow2, etc....","title":"GKSfromWorflows"},{"location":"GKSfromWorflows/#gksfromworflows","text":"GKSfromWorflows uses the python script galaxykickstart_from_workflow.py to quickly generate a GalaxyKickStart use case from one or several workflow files ( .ga , note that these files must have been generated with galaxy = release_16.04)","title":"GKSfromWorflows"},{"location":"GKSfromWorflows/#from-galaxykickstartscripts-run","text":"python galaxykickstart_from_workflow.py --help Then python galaxykickstart_from_workflow.py -w workflow1.ga workflow2.ga ... -l Panel_label This creates: An inventory file GKSfromWorkflow in the inventory_files folder A group_vars file GKSfromWorkflow in the group_vars folder A folder GKSfromWorkflow in the folder extra-files which will contain a copy of the workflow1.ga, workflow2.ga, ... files, plus a GKSfromWorkflow_tool_list.yml file that contains a yml description of all tools used in the workflows. Note that running galaxykickstart_from_workflow.py overwrites these folders and files if they exist from a previous script run.","title":"From GalaxyKickStart/scripts, run"},{"location":"GKSfromWorflows/#adapt-the-created-inventory-file","text":"Before running ansible-playbook, you have just to adapt the GKSfromWorkflow inventory_file inventory_files/GKSfromWorkflow to your own network settings (the file is preconfigured for running locally ansible-playbook on your target machine). As usual, you may also tune the group_vars/all file.","title":"Adapt the created inventory file"},{"location":"GKSfromWorflows/#run-the-playbook","text":"cd GalaxyKickStart ansible-galaxy install -r requirements_roles.yml -p roles ansible-playbook -i inventory_files/GKSfromWorkflow galaxy.yml Check your running Galaxy instance after completion of the playbook. It contains the preinstalled tools as well as the workflow1, workflow2, etc....","title":"Run the playbook"},{"location":"about/","text":"GalaxyKickStart GalaxyKickStart is an Ansible playbook designed to help you get one or more production-ready Galaxy servers based on Ubuntu within minutes, and to maintain these servers. Required ansible version = 2.4.0.0 Optionally, instances can be pre-loaded with tools and workflows. The playbook has been tested on Cloud Machines Vagrant Boxes Physical Servers Docker. GalaxyKickStart has been developed at the ARTbio platform and contains roles developed by the Galaxy team . List of roles included in this playbook ensure_postrgesql_up natefoo-postgresql_objects galaxy-os role galaxy role miniconda-role galaxy-extras role galaxy-trackster role galaxy-tools role","title":"What is GalaxyKickStart"},{"location":"about/#galaxykickstart","text":"GalaxyKickStart is an Ansible playbook designed to help you get one or more production-ready Galaxy servers based on Ubuntu within minutes, and to maintain these servers.","title":"GalaxyKickStart"},{"location":"about/#required-ansible-version-2400","text":"Optionally, instances can be pre-loaded with tools and workflows. The playbook has been tested on Cloud Machines Vagrant Boxes Physical Servers Docker. GalaxyKickStart has been developed at the ARTbio platform and contains roles developed by the Galaxy team .","title":"Required ansible version &gt;= 2.4.0.0"},{"location":"about/#list-of-roles-included-in-this-playbook","text":"ensure_postrgesql_up natefoo-postgresql_objects galaxy-os role galaxy role miniconda-role galaxy-extras role galaxy-trackster role galaxy-tools role","title":"List of roles included in this playbook"},{"location":"available_roles/","text":"ansible-postgresql-objects role ensure_postgresql_up role galaxy-os role miniconda role galaxy role galaxy-extras role ansible-trackster role galaxy-tools role","title":"Available roles"},{"location":"available_variables/","text":"","title":"Available variables"},{"location":"customizations/","text":"Customising the playbook We strongly encourage users to read the ansible inventory documentation first. Most settings should be editable without modifying the playbook directly, instead variables can be set in group_vars and host vars. The playbook comes with an example inventory file hosts . [artimed] localhost ansible_ssh_user= root ansible_ssh_private_key_file= ~/.ssh/id_rsa [travis_bioblend] localhost ansible_connection=local [aws] # Put you aws IP and key here to make FTP work in the default VPC. # If you want further group-specific variables, put the host in these groups as well [e.g artimed]. [artimed] , [travis_bioblend] and [aws] are predefined groups. Any host (here we only have localhost) that is added to one or multiple groups will have the corresponding group variables applied. Group variables are defined in group_vars/[name of the group] and default variables are found in group_vars/all . All variables defined in group_vars/all are overwritten in group_vars/[name of the group] . For instance the variable proftpd_nat_masquerade is set to false in group_vars/all , while hosts in the [aws] group apply the [aws] group variables which set proftpd_nat_masquerade to true, so that hosts in the aws group will have this aws-specific setting applied. Any combination of groups may be used. If you want to apply any of the changes you made to the variables you need to run the playbook again, making sure that the host you are targeting is in the right group. The simplest way to do so is to use an inventory file that only contains the group and the host you wish to target. If this is for example the group metavisitor, and you target the host localhost, your inventory file should look like this: [metavisitor] localhost You can then run the playbook as usual: ansible-playbook --inventory-file= your_inventory_file galaxy.yml Important variables We aimed for this playbook to be reusable. We therefore made most variables configurable. The group_vars/all file contains the variables we have chosen as defaults. You may override them either in this file or you can use ansible group variables to selectively set the variables for certain hosts/groups. See the ansible documentation about group variables for details. These most important variables are: ansible_ssh_user - The login name used to access the target. ansible_ssh_private_key_file - The ssh private key used to access the target. install_galaxy - True for install a Galaxy instance. install_tools - True for install the NGS tools. run_data_manager - True for run the data manager procedure. galaxy_user_name - The Operating System user name for galaxy process. galaxy_server_dir - The home of Operating System user for galaxy process. galaxy_admin - The admin galaxy user. galaxy_admin_pw - The admin galaxy password. default_admin_api_key - The api key for tool installation and download reference genomes throught galaxy data managers. To be removed in production. galaxy_tool_list - The files that constants the list of tools to be installed. galaxy_data_managers - The reference genomes and indexes to be load and build. galaxy_data - The persistent directory where the galaxy config and database directories will be installed or will be recovered. galaxy_database - The persistent directory where postgresql will be installed or will be recovered. galaxy_db - Connection string for galaxy-postgresql. galaxy_changeset_id - The release of Galaxy to be installed (master, dev or release_xx_xx).","title":"Customising the playbook"},{"location":"customizations/#customising-the-playbook","text":"We strongly encourage users to read the ansible inventory documentation first. Most settings should be editable without modifying the playbook directly, instead variables can be set in group_vars and host vars. The playbook comes with an example inventory file hosts . [artimed] localhost ansible_ssh_user= root ansible_ssh_private_key_file= ~/.ssh/id_rsa [travis_bioblend] localhost ansible_connection=local [aws] # Put you aws IP and key here to make FTP work in the default VPC. # If you want further group-specific variables, put the host in these groups as well [e.g artimed]. [artimed] , [travis_bioblend] and [aws] are predefined groups. Any host (here we only have localhost) that is added to one or multiple groups will have the corresponding group variables applied. Group variables are defined in group_vars/[name of the group] and default variables are found in group_vars/all . All variables defined in group_vars/all are overwritten in group_vars/[name of the group] . For instance the variable proftpd_nat_masquerade is set to false in group_vars/all , while hosts in the [aws] group apply the [aws] group variables which set proftpd_nat_masquerade to true, so that hosts in the aws group will have this aws-specific setting applied. Any combination of groups may be used. If you want to apply any of the changes you made to the variables you need to run the playbook again, making sure that the host you are targeting is in the right group. The simplest way to do so is to use an inventory file that only contains the group and the host you wish to target. If this is for example the group metavisitor, and you target the host localhost, your inventory file should look like this: [metavisitor] localhost You can then run the playbook as usual: ansible-playbook --inventory-file= your_inventory_file galaxy.yml","title":"Customising the playbook"},{"location":"customizations/#important-variables","text":"We aimed for this playbook to be reusable. We therefore made most variables configurable. The group_vars/all file contains the variables we have chosen as defaults. You may override them either in this file or you can use ansible group variables to selectively set the variables for certain hosts/groups. See the ansible documentation about group variables for details. These most important variables are: ansible_ssh_user - The login name used to access the target. ansible_ssh_private_key_file - The ssh private key used to access the target. install_galaxy - True for install a Galaxy instance. install_tools - True for install the NGS tools. run_data_manager - True for run the data manager procedure. galaxy_user_name - The Operating System user name for galaxy process. galaxy_server_dir - The home of Operating System user for galaxy process. galaxy_admin - The admin galaxy user. galaxy_admin_pw - The admin galaxy password. default_admin_api_key - The api key for tool installation and download reference genomes throught galaxy data managers. To be removed in production. galaxy_tool_list - The files that constants the list of tools to be installed. galaxy_data_managers - The reference genomes and indexes to be load and build. galaxy_data - The persistent directory where the galaxy config and database directories will be installed or will be recovered. galaxy_database - The persistent directory where postgresql will be installed or will be recovered. galaxy_db - Connection string for galaxy-postgresql. galaxy_changeset_id - The release of Galaxy to be installed (master, dev or release_xx_xx).","title":"Important variables"},{"location":"examples/","text":"","title":"Examples"},{"location":"faq/","text":"Why does the playbook fail? Make sure that you are on ansible version =2.1. You can check your ansible version by typing: ansible --version What is the username and password of the galaxy admin account ? Username and password of the galaxy account are controlled by the variables galaxy_admin and galaxy_admin_pw and default to admin@galaxy.org and admin (Defaults are defined in group_vars/all). This should be changed in the group or host variables for the host you are working on. If you have a host in the mygroup group, you can edit group_vars/my_group and set galaxy_admin: new_admin@email.com galaxy_admin_pw: new_password As with each change, run the playbook again. How can I set up GalaxyKickStart behind a proxy? Many commandline utilities can be configured to use a proxy by setting the http_proxy and https_proxy environment variables. Tasks launched by ansible will only see these environment variables if ansible sets these variables for the task. We have included a global proxy_env variable in the galaxy.yml playbook. You can set the content of this variable in your inventory or group variables (See Customizing the playbook for details on how to define variable). To use the proxy at http://proxy.bos.example.com:8080 define the variable proxy_env like so: proxy_env: http_proxy: http://proxy.bos.example.com:8080 https_proxy: http://proxy.bos.example.com:8080 no_proxy: localhost,127.0.0.0,127.0.1.1,127.0.1.1,local.home Adresses that should not be contacted through a proxy should be listed in the no_proxy variable. An example can be found in group_vars/proxy.","title":"Frequently asked questions"},{"location":"faq/#why-does-the-playbook-fail","text":"Make sure that you are on ansible version =2.1. You can check your ansible version by typing: ansible --version","title":"Why does the playbook fail?"},{"location":"faq/#what-is-the-username-and-password-of-the-galaxy-admin-account","text":"Username and password of the galaxy account are controlled by the variables galaxy_admin and galaxy_admin_pw and default to admin@galaxy.org and admin (Defaults are defined in group_vars/all). This should be changed in the group or host variables for the host you are working on. If you have a host in the mygroup group, you can edit group_vars/my_group and set galaxy_admin: new_admin@email.com galaxy_admin_pw: new_password As with each change, run the playbook again.","title":"What is the username and password of the galaxy admin account ?"},{"location":"faq/#how-can-i-set-up-galaxykickstart-behind-a-proxy","text":"Many commandline utilities can be configured to use a proxy by setting the http_proxy and https_proxy environment variables. Tasks launched by ansible will only see these environment variables if ansible sets these variables for the task. We have included a global proxy_env variable in the galaxy.yml playbook. You can set the content of this variable in your inventory or group variables (See Customizing the playbook for details on how to define variable). To use the proxy at http://proxy.bos.example.com:8080 define the variable proxy_env like so: proxy_env: http_proxy: http://proxy.bos.example.com:8080 https_proxy: http://proxy.bos.example.com:8080 no_proxy: localhost,127.0.0.0,127.0.1.1,127.0.1.1,local.home Adresses that should not be contacted through a proxy should be listed in the no_proxy variable. An example can be found in group_vars/proxy.","title":"How can I set up GalaxyKickStart behind a proxy?"},{"location":"getting_started/","text":"Getting Started You need git installed Make sure that you have a recent version of Ansible installed The playbook has been tested with the Ansible stable version 2.4 Install Ansible with pip A simple way to install the latest Ansible version is using pip : Install pip sudo apt-get update -y sudo apt-get -y install python-pip python-dev Ensure you have recent pip version installed (sudo -i pip install upgrade pip maybe necessary) pip upgrade: sudo pip install -U pip $ pip --version pip 9.0.1 from /usr/local/lib/python2.7/site-packages (python 2.7) Then pip install ansible==2.4 Install Ansible with apt Alternatively, Ansible may be installed with the Apt package manager (Ubuntu): sudo -i apt-get install software-properties-common apt-add-repository ppa:ansible/ansible apt-get update apt-get install ansible Getting the playbook GalaxyKickStart is hosted on github and uses a number of dependent Ansible roles that need to be downloaded as part of the installation step: git clone https://github.com/ARTbio/GalaxyKickStart.git cd GalaxyKickStart ansible-galaxy install -r requirements_roles.yml -p roles The playbook (here galaxy.yml ) should be in the GalaxyKickStart folder. ls CONTRIBUTORS.md Vagrantfile docs inventory_files roles Dockerfile ansible.cfg extra-files mkdocs.yml scripts LICENSE.txt deploy.sh galaxy.yml pre-commit.sh startup.sh README.md dockerfiles group_vars requirements_roles.yml templates Deploying galaxy-kickstart on remote machines. Inside the inventory_files folder, you will find a number of inventory files. This is an example of inventory taken from the artimed inventory file. [artimed] localhost ansible_ssh_user= root ansible_ssh_private_key_file= ~/.ssh/id_rsa ... Here [artimed] is a group, that contains a machine called localhost. The variables defined in group_vars/artimed will be applied to this host. Ansible will connect by ssh to this machine, using the ssh key in ~/.ssh/id_rsa . If you would like to run this playbook on a remote machine by ssh (currently needs to be a debian-type machine), create a new inventory, and change localhost to the IP address of that machine. ansible_ssh_user= user controls under which username to connect to this machine. This user needs to have sudo rights. Then, run the plabook by typing: ansible-playbook --inventory-file inventory_files/ your_inventory_file galaxy.yml You can put multiple machines in your inventory. If you run the playbook a second time, the process will be much faster, since steps that have already been executed will be skipped. Whenever you change a variable (see customizations ), you need to run the playbook again. Deploying galaxy-kickstart on specified clouds Inside the repository you will find a file called inventory_files/cloud . This file serves as an example hosts file for how to deploy galaxy-kickstart on Google Compute Engine (GCE), Amazon Web Services(aws), and Jetstream (OpenStack). Please note that the ansible_user variable in the file changes for each remote target . If you are wanting to use this playbook on a cloud other than the ones listed below, you will need to update the inventory to add a new section header for the respective target. If this happens to be a cloud setup, make sure to add the section header under [cloud_setup:children] . Specifications for each remote target: OpenStack Image needed to deploy on Jetstream : Ubuntu 14.04.3 Development (jetstream image id: d7fe3289-943f-4417-90e8-8a6b171677ca) Inventory: remote host IP anisble_ssh_user=\"root\" ansible_ssh_private_key_file=\" path/to/your/private/key \" GCE Image needed to deploy galaxy-kickstart: Ubuntu 14.04 LTS Inventory: remote host IP anisble_ssh_user=\"ubuntu\" ansible_ssh_private_key_file=\" path/to/your/private/key \" AWS Image needed to deploy galaxy-kickstart: Ubuntu Server 14.04 LTS (HVM), SSD Volume Type - ami-2d39803a Inventory: target Amazon Web Services IP address ansible_ssh_user=\"ubuntu\" ansible_ssh_private_key_file=\" path/to/your/aws/private/key \" Deploying galaxy-kickstart behind a proxy See How can I set up GalaxyKickStart behind a proxy?","title":"Getting started"},{"location":"getting_started/#getting-started","text":"","title":"Getting Started"},{"location":"getting_started/#you-need-git-installed","text":"","title":"You need git installed"},{"location":"getting_started/#make-sure-that-you-have-a-recent-version-of-ansible-installed","text":"The playbook has been tested with the Ansible stable version 2.4","title":"Make sure that you have a recent version of Ansible installed"},{"location":"getting_started/#install-ansible-with-pip","text":"A simple way to install the latest Ansible version is using pip :","title":"Install Ansible with pip"},{"location":"getting_started/#install-pip","text":"sudo apt-get update -y sudo apt-get -y install python-pip python-dev Ensure you have recent pip version installed (sudo -i pip install upgrade pip maybe necessary) pip upgrade: sudo pip install -U pip $ pip --version pip 9.0.1 from /usr/local/lib/python2.7/site-packages (python 2.7) Then pip install ansible==2.4","title":"Install pip"},{"location":"getting_started/#install-ansible-with-apt","text":"Alternatively, Ansible may be installed with the Apt package manager (Ubuntu): sudo -i apt-get install software-properties-common apt-add-repository ppa:ansible/ansible apt-get update apt-get install ansible","title":"Install Ansible with apt"},{"location":"getting_started/#getting-the-playbook","text":"GalaxyKickStart is hosted on github and uses a number of dependent Ansible roles that need to be downloaded as part of the installation step: git clone https://github.com/ARTbio/GalaxyKickStart.git cd GalaxyKickStart ansible-galaxy install -r requirements_roles.yml -p roles The playbook (here galaxy.yml ) should be in the GalaxyKickStart folder. ls CONTRIBUTORS.md Vagrantfile docs inventory_files roles Dockerfile ansible.cfg extra-files mkdocs.yml scripts LICENSE.txt deploy.sh galaxy.yml pre-commit.sh startup.sh README.md dockerfiles group_vars requirements_roles.yml templates","title":"Getting the playbook"},{"location":"getting_started/#deploying-galaxy-kickstart-on-remote-machines","text":"Inside the inventory_files folder, you will find a number of inventory files. This is an example of inventory taken from the artimed inventory file. [artimed] localhost ansible_ssh_user= root ansible_ssh_private_key_file= ~/.ssh/id_rsa ... Here [artimed] is a group, that contains a machine called localhost. The variables defined in group_vars/artimed will be applied to this host. Ansible will connect by ssh to this machine, using the ssh key in ~/.ssh/id_rsa . If you would like to run this playbook on a remote machine by ssh (currently needs to be a debian-type machine), create a new inventory, and change localhost to the IP address of that machine. ansible_ssh_user= user controls under which username to connect to this machine. This user needs to have sudo rights. Then, run the plabook by typing: ansible-playbook --inventory-file inventory_files/ your_inventory_file galaxy.yml You can put multiple machines in your inventory. If you run the playbook a second time, the process will be much faster, since steps that have already been executed will be skipped. Whenever you change a variable (see customizations ), you need to run the playbook again.","title":"Deploying galaxy-kickstart on remote machines."},{"location":"getting_started/#deploying-galaxy-kickstart-on-specified-clouds","text":"Inside the repository you will find a file called inventory_files/cloud . This file serves as an example hosts file for how to deploy galaxy-kickstart on Google Compute Engine (GCE), Amazon Web Services(aws), and Jetstream (OpenStack). Please note that the ansible_user variable in the file changes for each remote target . If you are wanting to use this playbook on a cloud other than the ones listed below, you will need to update the inventory to add a new section header for the respective target. If this happens to be a cloud setup, make sure to add the section header under [cloud_setup:children] . Specifications for each remote target: OpenStack Image needed to deploy on Jetstream : Ubuntu 14.04.3 Development (jetstream image id: d7fe3289-943f-4417-90e8-8a6b171677ca) Inventory: remote host IP anisble_ssh_user=\"root\" ansible_ssh_private_key_file=\" path/to/your/private/key \" GCE Image needed to deploy galaxy-kickstart: Ubuntu 14.04 LTS Inventory: remote host IP anisble_ssh_user=\"ubuntu\" ansible_ssh_private_key_file=\" path/to/your/private/key \" AWS Image needed to deploy galaxy-kickstart: Ubuntu Server 14.04 LTS (HVM), SSD Volume Type - ami-2d39803a Inventory: target Amazon Web Services IP address ansible_ssh_user=\"ubuntu\" ansible_ssh_private_key_file=\" path/to/your/aws/private/key \"","title":"Deploying galaxy-kickstart on specified clouds"},{"location":"getting_started/#deploying-galaxy-kickstart-behind-a-proxy","text":"See How can I set up GalaxyKickStart behind a proxy?","title":"Deploying galaxy-kickstart behind a proxy"},{"location":"installing_tools_and_workflows/","text":"Installing tools This playbook includes the ansible-galaxy-tools role which can be used to install tools and workflows into galaxy instances using the bioblend api. Creating a tool_list.yml file To install tools, you will need to prepare a list of tools in yaml format. A an example of a a tool list can be found in here tools: - name: blast_to_scaffold owner: drosofff revisions: tool_panel_section_label: Metavisitor tool_shed_url: https://toolshed.g2.bx.psu.edu/ - name: blastx_to_scaffold owner: drosofff revisions: tool_panel_section_label: Metavisitor tool_shed_url: https://toolshed.g2.bx.psu.edu/ - name: bowtie2 owner: devteam revisions: - 019c2a81547a tool_panel_section_label: Metavisitor tool_shed_url: https://toolshed.g2.bx.psu.edu/ when the revision is empty, the latest available revision will be installed. tool_panel_section_label will determine the tool panel section where the tools will be found. Obtaining a tool_list.yml file We can also obtain a tool list from a runnning galaxy instance. Note that for server running a galaxy release 16.04, you need a galaxy API keys and bioblend. A script is included in the extra-files directory. python get_tool_yml_from_gi.py --galaxy my_galaxy_url --api-key my_admin_api_key --output-file my_tool_list.yml Adding a tool_list.yml file to a group_variable files Group variable files are in the group_vars directory. If you would like to install tools, you need to reference the tool_list.yml in the group variable file. We typically place additional files in the extra-files/ hostname / hostname _tool_list.yml file. If you would like to add tools to a group that is called metavisitor edit group_vars/metavisitor and add these lines: install_tools: true galaxy_tools_tool_list: extra-files/metavisitor/metavisitor_tool_list.yml Installing workflows You can also make sure that workflows are available after running the playbook. As with tools, place the workflows in extra-files/ hostname / hostname workflow_name .ga Add these lines to the corresponding group_var file: galaxy_tools_install_workflows: true galaxy_tools_workflows: - extra-files/metavisitor/Galaxy-Workflow-create_model.ga - extra-files/metavisitor/Galaxy-Workflow-separate_host_and_virus_reads.ga - extra-files/metavisitor/Galaxy-Workflow-standart_metavisitor_workflow_(input__clipped_dataset).ga - extra-files/metavisitor/Galaxy-Workflow-Metavisitor_Test_case_1-1_Guided.ga - extra-files/metavisitor/Galaxy-Workflow-Metavisitor_Test_case_1-2_Guided.ga - extra-files/metavisitor/Galaxy-Workflow-Metavisitor_Test_case_1-3_Guided.ga - extra-files/metavisitor/Galaxy-Workflow-Meta-visitor__test_case_Nora_virus,_REMAPPING.ga Running the playbook As per usual, run the playbook with an inventory file that maps your target machine to the metavisitor group. If the target is localhost, your inventory file should look ike this: [metavisitor] localhost then run the playbook like so: ansible-playbook --inventory-file= your_inventory_file galaxy.yml","title":"Installing tools and workflows"},{"location":"installing_tools_and_workflows/#installing-tools","text":"This playbook includes the ansible-galaxy-tools role which can be used to install tools and workflows into galaxy instances using the bioblend api.","title":"Installing tools"},{"location":"installing_tools_and_workflows/#creating-a-tool_listyml-file","text":"To install tools, you will need to prepare a list of tools in yaml format. A an example of a a tool list can be found in here tools: - name: blast_to_scaffold owner: drosofff revisions: tool_panel_section_label: Metavisitor tool_shed_url: https://toolshed.g2.bx.psu.edu/ - name: blastx_to_scaffold owner: drosofff revisions: tool_panel_section_label: Metavisitor tool_shed_url: https://toolshed.g2.bx.psu.edu/ - name: bowtie2 owner: devteam revisions: - 019c2a81547a tool_panel_section_label: Metavisitor tool_shed_url: https://toolshed.g2.bx.psu.edu/ when the revision is empty, the latest available revision will be installed. tool_panel_section_label will determine the tool panel section where the tools will be found.","title":"Creating a tool_list.yml file"},{"location":"installing_tools_and_workflows/#obtaining-a-tool_listyml-file","text":"We can also obtain a tool list from a runnning galaxy instance. Note that for server running a galaxy release 16.04, you need a galaxy API keys and bioblend. A script is included in the extra-files directory. python get_tool_yml_from_gi.py --galaxy my_galaxy_url --api-key my_admin_api_key --output-file my_tool_list.yml","title":"Obtaining a tool_list.yml file"},{"location":"installing_tools_and_workflows/#adding-a-tool_listyml-file-to-a-group_variable-files","text":"Group variable files are in the group_vars directory. If you would like to install tools, you need to reference the tool_list.yml in the group variable file. We typically place additional files in the extra-files/ hostname / hostname _tool_list.yml file. If you would like to add tools to a group that is called metavisitor edit group_vars/metavisitor and add these lines: install_tools: true galaxy_tools_tool_list: extra-files/metavisitor/metavisitor_tool_list.yml","title":"Adding a tool_list.yml file to a group_variable files"},{"location":"installing_tools_and_workflows/#installing-workflows","text":"You can also make sure that workflows are available after running the playbook. As with tools, place the workflows in extra-files/ hostname / hostname workflow_name .ga Add these lines to the corresponding group_var file: galaxy_tools_install_workflows: true galaxy_tools_workflows: - extra-files/metavisitor/Galaxy-Workflow-create_model.ga - extra-files/metavisitor/Galaxy-Workflow-separate_host_and_virus_reads.ga - extra-files/metavisitor/Galaxy-Workflow-standart_metavisitor_workflow_(input__clipped_dataset).ga - extra-files/metavisitor/Galaxy-Workflow-Metavisitor_Test_case_1-1_Guided.ga - extra-files/metavisitor/Galaxy-Workflow-Metavisitor_Test_case_1-2_Guided.ga - extra-files/metavisitor/Galaxy-Workflow-Metavisitor_Test_case_1-3_Guided.ga - extra-files/metavisitor/Galaxy-Workflow-Meta-visitor__test_case_Nora_virus,_REMAPPING.ga","title":"Installing workflows"},{"location":"installing_tools_and_workflows/#running-the-playbook","text":"As per usual, run the playbook with an inventory file that maps your target machine to the metavisitor group. If the target is localhost, your inventory file should look ike this: [metavisitor] localhost then run the playbook like so: ansible-playbook --inventory-file= your_inventory_file galaxy.yml","title":"Running the playbook"},{"location":"metavisitor/","text":"Metavisitor is a set of Galaxy tools and workflows to detect and reconstruct viral genomes from complex deep sequence datasets. Documentation on Metavisitor installation using GalaxyKickStart is available in the Metavisitor manual","title":"Metavisitor"},{"location":"examples/docker/","text":"Building and deploying galaxy-kickstart in docker Requirements You need to have docker installed and configured for your user. The repository comes with various Dockerfiles that can be used to configure a deployment using Docker, or you can start with a pre-built docker image. Running images from the dockerhub You can obtain a pre-built docker image from the dockerhub: docker pull artbio/galaxy-kickstart-base Start the image and serve it on port 8080 of your local machine in the standard docker way: CID=`docker run -d -p 8080:80 artbio/galaxy-kickstart-base` -p 8080:80 will forward requests to nginx inside the container running on port 80. Runtime changes to pre-built docker images If you wish to reach the container on a subdirectory, add -e NGINX_GALAXY_LOCATION=\"/my-subdirectory\" to the docker call and galaxy will be served at http://127.0.0.1:8080/my-subdirectory . We recommend changing the default admin user as well, so the command becomes: CID=`docker run -d -e NGINX_GALAXY_LOCATION= /my-subdirectory -e GALAXY_CONFIG_ADMIN_USERS=admin@artbio.fr -p 8080:80 artbio/galaxy-kickstart-base` Commit changed containers to new images As with standard docker containers, you can change, tag and commit running containers when you have configured them to your liking: Commit the changes to my-new-image docker commit $CID my-new-image Stop and remove the original container: docker stop $CID docker rm $CID Start the new container: CID=`docker run -d -e NGINX_GALAXY_LOCATION= /my-subdirectory -e GALAXY_CONFIG_ADMIN_USERS=admin@artbio.fr -p 8080:80 my-new-image` Persisting to disk All changes made to the container are by default ephemeral; if you remove the container, the changes are gone. To persist data (this includes the postgresql database, galaxy's config files and your user data), mount a Volume into the containers /export folder. Due to the persistance mechanism (we use bind-mounts inside the container), you need to privilege the container. Assuming you would like to mount your local /data folder, run CID=`docker run -d --privileged -v /data:/export -p 8080:80 my-new-image` This will run through the persistence tags of the galaxy.yml and export the required files to /export (now on your machine's /data). From the new location the files are being bind-mounted back into their original location.","title":"Docker"},{"location":"examples/docker/#building-and-deploying-galaxy-kickstart-in-docker","text":"","title":"Building and deploying galaxy-kickstart in docker"},{"location":"examples/docker/#requirements","text":"You need to have docker installed and configured for your user. The repository comes with various Dockerfiles that can be used to configure a deployment using Docker, or you can start with a pre-built docker image.","title":"Requirements"},{"location":"examples/docker/#running-images-from-the-dockerhub","text":"You can obtain a pre-built docker image from the dockerhub: docker pull artbio/galaxy-kickstart-base Start the image and serve it on port 8080 of your local machine in the standard docker way: CID=`docker run -d -p 8080:80 artbio/galaxy-kickstart-base` -p 8080:80 will forward requests to nginx inside the container running on port 80.","title":"Running images from the dockerhub"},{"location":"examples/docker/#runtime-changes-to-pre-built-docker-images","text":"If you wish to reach the container on a subdirectory, add -e NGINX_GALAXY_LOCATION=\"/my-subdirectory\" to the docker call and galaxy will be served at http://127.0.0.1:8080/my-subdirectory . We recommend changing the default admin user as well, so the command becomes: CID=`docker run -d -e NGINX_GALAXY_LOCATION= /my-subdirectory -e GALAXY_CONFIG_ADMIN_USERS=admin@artbio.fr -p 8080:80 artbio/galaxy-kickstart-base`","title":"Runtime changes to pre-built docker images"},{"location":"examples/docker/#commit-changed-containers-to-new-images","text":"As with standard docker containers, you can change, tag and commit running containers when you have configured them to your liking: Commit the changes to my-new-image docker commit $CID my-new-image Stop and remove the original container: docker stop $CID docker rm $CID Start the new container: CID=`docker run -d -e NGINX_GALAXY_LOCATION= /my-subdirectory -e GALAXY_CONFIG_ADMIN_USERS=admin@artbio.fr -p 8080:80 my-new-image`","title":"Commit changed containers to new images"},{"location":"examples/docker/#persisting-to-disk","text":"All changes made to the container are by default ephemeral; if you remove the container, the changes are gone. To persist data (this includes the postgresql database, galaxy's config files and your user data), mount a Volume into the containers /export folder. Due to the persistance mechanism (we use bind-mounts inside the container), you need to privilege the container. Assuming you would like to mount your local /data folder, run CID=`docker run -d --privileged -v /data:/export -p 8080:80 my-new-image` This will run through the persistence tags of the galaxy.yml and export the required files to /export (now on your machine's /data). From the new location the files are being bind-mounted back into their original location.","title":"Persisting to disk"},{"location":"examples/vagrant/","text":"Deploying galaxy-kickstart on local virtual machine (VM) using vagrant. GalaxyKickStart is designed to be flexible and powerful, but for demonstration purposes we start a simple vagrant box that runs this playbook. Following these instructions will not change the host system. Alternatively, see examples/docker for running the playbook in docker, or getting started for running the playbook on local or remote machines. Requirements To follow the examples ansible , vagrant and git need to be installed. Running the playbook on a Virtual Machine The Vagrantfile describes a Virtual Machine (VM) that is based on Ubuntu 14.04 (codename trusty). VAGRANTFILE_API_VERSION = 2 Vagrant.configure(VAGRANTFILE_API_VERSION) do |config| config.vm.box = ubuntu/trusty64 config.vm.network forwarded_port , guest: 80, host: 8080 config.vm.network forwarded_port , guest: 21, host: 2121 config.vm.provider virtualbox do |v| v.memory = 4096 end config.vm.provision ansible do |ansible| ansible.extra_vars = { ntp_server: pool.ntp.org , ansible_ssh_user: 'vagrant' } ansible.verbose = 'vvvv' ansible.playbook = galaxy.yml end end By default, port 8080 will be forwarded to port 80, and port 2121 will be forwarded to port 21 (for FTP), and 4096 MB of memory will be attributed to the VM. Enter the playbook directory cd GalaxyKickStart and type vagrant up to download a VM image and run the galaxy.yml playbook. This will take a while. Once finished, you should find a running Galaxy Instance on http://localhost:8080 . If you would like to see the internals of the VM, you can log into the machine by typing vagrant ssh . vagrant up makes use of the ansible provisioner and is equivalent of starting a vagrant machine without the ansible provisioner and running ansible through an ssh connection to the vagrant machine (which listens by default on port 2222) The hosts inventory file contains an example for directly pointing ansible to the vagrant machine. Uncomment the vagrant specific lines and comment or remove the remaining lines: #[artimed] #localhost ansible_ssh_user= root ansible_ssh_private_key_file= ~/.ssh/id_rsa #[travis_bioblend] #localhost ansible_connection=local # Uncomment the 2 lines below to point ansible to a local vagrant machine. [all] localhost ansible_user= vagrant ansible_port=2222 ansible_private_key_file=.vagrant/machines/default/virtualbox/private_key #[aws] # Put you aws IP and key here to make FTP work in the default VPC. # If you want further group-specific variables, put the host in these groups as well [e.g artimed]. To run the playbook again, type ansible-playbook --inventory-file= your_inventory galaxy.yml Cleaning up The VM image and various config files have been written to the .vagrant folder. Type vagrant halt to stop the running instance and vagrant destroy to remove the VM, and then delete the .vagrant folder.","title":"Deploying galaxy-kickstart on local virtual machine (VM) using vagrant."},{"location":"examples/vagrant/#deploying-galaxy-kickstart-on-local-virtual-machine-vm-using-vagrant","text":"GalaxyKickStart is designed to be flexible and powerful, but for demonstration purposes we start a simple vagrant box that runs this playbook. Following these instructions will not change the host system. Alternatively, see examples/docker for running the playbook in docker, or getting started for running the playbook on local or remote machines.","title":"Deploying galaxy-kickstart on local virtual machine (VM) using vagrant."},{"location":"examples/vagrant/#requirements","text":"To follow the examples ansible , vagrant and git need to be installed.","title":"Requirements"},{"location":"examples/vagrant/#running-the-playbook-on-a-virtual-machine","text":"The Vagrantfile describes a Virtual Machine (VM) that is based on Ubuntu 14.04 (codename trusty). VAGRANTFILE_API_VERSION = 2 Vagrant.configure(VAGRANTFILE_API_VERSION) do |config| config.vm.box = ubuntu/trusty64 config.vm.network forwarded_port , guest: 80, host: 8080 config.vm.network forwarded_port , guest: 21, host: 2121 config.vm.provider virtualbox do |v| v.memory = 4096 end config.vm.provision ansible do |ansible| ansible.extra_vars = { ntp_server: pool.ntp.org , ansible_ssh_user: 'vagrant' } ansible.verbose = 'vvvv' ansible.playbook = galaxy.yml end end By default, port 8080 will be forwarded to port 80, and port 2121 will be forwarded to port 21 (for FTP), and 4096 MB of memory will be attributed to the VM. Enter the playbook directory cd GalaxyKickStart and type vagrant up to download a VM image and run the galaxy.yml playbook. This will take a while. Once finished, you should find a running Galaxy Instance on http://localhost:8080 . If you would like to see the internals of the VM, you can log into the machine by typing vagrant ssh . vagrant up makes use of the ansible provisioner and is equivalent of starting a vagrant machine without the ansible provisioner and running ansible through an ssh connection to the vagrant machine (which listens by default on port 2222) The hosts inventory file contains an example for directly pointing ansible to the vagrant machine. Uncomment the vagrant specific lines and comment or remove the remaining lines: #[artimed] #localhost ansible_ssh_user= root ansible_ssh_private_key_file= ~/.ssh/id_rsa #[travis_bioblend] #localhost ansible_connection=local # Uncomment the 2 lines below to point ansible to a local vagrant machine. [all] localhost ansible_user= vagrant ansible_port=2222 ansible_private_key_file=.vagrant/machines/default/virtualbox/private_key #[aws] # Put you aws IP and key here to make FTP work in the default VPC. # If you want further group-specific variables, put the host in these groups as well [e.g artimed]. To run the playbook again, type ansible-playbook --inventory-file= your_inventory galaxy.yml","title":"Running the playbook on a Virtual Machine"},{"location":"examples/vagrant/#cleaning-up","text":"The VM image and various config files have been written to the .vagrant folder. Type vagrant halt to stop the running instance and vagrant destroy to remove the VM, and then delete the .vagrant folder.","title":"Cleaning up"}]}